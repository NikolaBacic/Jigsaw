{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pickled-glove840b300d-for-10sec-loading', 'jigsaw-unintended-bias-in-toxicity-classification']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.layers import Embedding, Bidirectional, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import Constant\n",
    "\n",
    "import time\n",
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    with open(path,'rb') as f:\n",
    "        emb_arr = pickle.load(f)\n",
    "    return emb_arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is really important. Although we put a lot of effort in making the preprocessing right there are stil some out of vocabulary words we could easily fix. One example I implement here is to try a lower/upper case version of a word if an embedding is not found, which sometimes gives us an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((max_features + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i <= max_features:\n",
    "            try:\n",
    "                embedding_matrix[i] = embedding_index[word]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    embedding_matrix[i] = embedding_index[word.lower()]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        embedding_matrix[i] = embedding_index[word.title()]\n",
    "                    except KeyError:\n",
    "                        unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_to_isolate = '.,?!-;*\"â€¦:â€”()%#$&_/@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â‰ â„¢ËˆÊŠÉ’âˆžÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\\x96\\x92â—Â£â™¥âž¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûžâ€ Î¼âœ’âž¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼Žâ¬…â„…Â»Ð’Ð°Ð²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—žâ–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹âž¡Â«Ï†â…“â€žâœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑÐ¸Ê¿âœ¨ã€‚É‘\\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜ŽâœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆŽâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼žÊ•ÉÌ£Î”â‚€âœžâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜žï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜'\n",
    "\n",
    "symbols_to_delete = '\\nðŸ•\\rðŸµðŸ˜‘\\xa0\\ue014\\t\\uf818\\uf04a\\xadðŸ˜¢ðŸ¶ï¸\\uf0e0ðŸ˜œðŸ˜ŽðŸ‘Š\\u200b\\u200eðŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±ðŸ˜ðŸ’–ðŸ’µÐ•ðŸ‘ŽðŸ˜€ðŸ˜‚\\u202a\\u202cðŸ”¥ðŸ˜„ðŸ»ðŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ðŸ˜‹ðŸ‘×©×œ×•××‘×™ðŸ˜±â€¼\\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\\u2009ðŸšŒá´µÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•\\u200fðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜××¢×›×—ðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼à®œðŸ˜–á´ ðŸš²â€ðŸ˜ŸðŸ˜ˆðŸ’ªðŸ™ðŸŽ¯ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡\\x7fðŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ðŸ™„ï¼¨ðŸ˜ \\ufeff\\u2028ðŸ˜‰ðŸ˜¤â›ºðŸ™‚\\u3000ØªØ­ÙƒØ³Ø©ðŸ‘®ðŸ’™ÙØ²Ø·ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜ž\\u2008ðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ª\\x08â€‘ðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨ðŸ™ƒðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šåœ°ç„è°·ÑƒÐ»ÐºÐ½ÐŸÐ¾ÐÐðŸ¾ðŸ•ðŸ˜†×”ðŸ”—ðŸš½æ­Œèˆžä¼ŽðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸Ð¼Ï…Ñ‚Ñ•â¤µðŸ†ðŸŽƒðŸ˜©\\u200aðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’ŽÑÐ¿Ñ€Ð´\\x95ðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™Œ\\u2002ðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰\\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\\x13ðŸš¬ðŸ¤“\\ue602ðŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª×ž×“×£× ×¨×š×¦×˜ðŸ˜’ÍðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“\\uf0b7\\uf04c\\x9f\\x10æˆéƒ½ðŸ˜£âºðŸ˜ŒðŸ¤‘ðŸŒðŸ˜¯ÐµÑ…ðŸ˜²á¼¸á¾¶á½ðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘\\u202dðŸ’¤ðŸ‡\\ue613å°åœŸè±†ðŸ¡â”â‰\\u202fðŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡è‹±æ–‡ðŸŒžðŸŽ²ãƒ¬ã‚¯ã‚µã‚¹ðŸ˜›å¤–å›½äººå…³ç³»Ð¡Ð±ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽÑŒÑ‹Ð³Ñä¸æ˜¯\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£ðŸ‘¿à¼¼ã¤à¼½ðŸ˜°á¸·Ð—Ð·â–±Ñ†ï¿¼ðŸ¤£å–æ¸©å“¥åŽè®®ä¼šä¸‹é™ä½ å¤±åŽ»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨Žéª—å­ðŸãƒ„ðŸŽ…\\x85ðŸºØ¢Ø¥Ø´Ø¡ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§Ð¹\\u2003ðŸš€ðŸ¤´Ê²ÑˆÑ‡Ð˜ÐžÐ Ð¤Ð”Ð¯ÐœÑŽÐ¶ðŸ˜ðŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ðŸ’¨åœ†æ˜Žå›­×§â„ðŸˆðŸ˜ºðŸŒâá»‡ðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦\\u200dð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–Ñ™ÐšÑ›ðŸ€ðŸ˜«ðŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªå¤©ä¸€å®¶âš²\\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹Êð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºÏ–\\u2000Ò¯Õ½á´¦áŽ¥Ò»Íº\\u2007Õ°\\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹ð€ð¥ðªðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸Ø¬íŒ¨í‹°ï¼·ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£Ñ–ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Ì¯ï´¿â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›â€’ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸å…¬å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ç¾Žð’‘ð’šð’ð‘´ðŸ¤™ðŸ’æ¬¢è¿Žæ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ð™«ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»ðŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—ðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·ð—¢ðŸ³ðŸ±ðŸ¬â¦ãƒžãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™Î¶ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šå¤šä¼¦ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ðŸ”«ðŸ‘å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ç‰ˆðŸ­ð‘¥ð‘¦ð‘§ï¼®ï¼§ðŸ‘£\\uf020ã£ðŸ‰Ñ„ðŸ’­ðŸŽ¥ÎžðŸ´ðŸ‘¨ðŸ¤³ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸ˜—ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ú†ÛŒð‘®ð—•ð—´ðŸ’êœ¥â²£â²ðŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ðŸ’Šã€Œã€\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ðŸ‡³ð’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«Ú©Î’ÏŽðŸ’¢ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´ðŸ’’âŠ˜È»ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ï½‡ï½’ï½Žï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆðŸ”­ã€ŠðŸŠðŸ\\uf10aáƒšÚ¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œã•ã‚ˆã†ãªã‚‰ðŸ”¼'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "\n",
    "isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "remove_dict = {ord(c):f'' for c in symbols_to_delete}\n",
    "\n",
    "\n",
    "def handle_punctuation(x):\n",
    "    x = x.translate(remove_dict)\n",
    "    x = x.translate(isolate_dict)\n",
    "    return x\n",
    "\n",
    "def handle_contractions(x):\n",
    "    x = tokenizer.tokenize(x)\n",
    "    return x\n",
    "\n",
    "def fix_quote(x):\n",
    "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "def preprocess(x):\n",
    "    x = handle_punctuation(x)\n",
    "    x = handle_contractions(x)\n",
    "    x = fix_quote(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets apply that preprocess function to our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1804874/1804874 [10:39<00:00, 2822.54it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97320/97320 [00:34<00:00, 2836.55it/s]\n"
     ]
    }
   ],
   "source": [
    "train['comment_text'] = train['comment_text'].progress_apply(lambda x:preprocess(x))\n",
    "test['comment_text'] = test['comment_text'].progress_apply(lambda x:preprocess(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing some of the most frequent word that doesn't have embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#  mapping: \n",
    "# \"tRump\" -> \"Trump\"\n",
    "# \"gov't\" -> \"government\"\n",
    "# \"Gov't\" -> \"Government\"\n",
    "# \"Qur'an\" -> \"Quran\"\n",
    "# \"Koncerned\" -> \"Concerned\"\n",
    "# \"y'all\" -> \"you guys\"\n",
    "# \"y'know\" -> you know\n",
    "\n",
    "def fix_1(x):\n",
    "    pattern1 = r\"\\btRump\\b\"\n",
    "    pattern2 = r\"\\bgov't\\b\"\n",
    "    pattern3 = r\"\\bGov't\\b\"\n",
    "    pattern4 = r\"\\bQur'an\\b\"\n",
    "    pattern5 = r\"\\bKoncerned\\b\"\n",
    "    pattern6 = r\"\\by'all\\b\"\n",
    "    pattern7 = r\"\\bgov'ts\\b\"\n",
    "    pattern8 = r\"\\by'know\\b\"\n",
    "\n",
    "    x = re.sub(pattern1,\"Trump\",x)\n",
    "    x = re.sub(pattern2,\"government\",x)\n",
    "    x = re.sub(pattern3,\"Government\",x)\n",
    "    x = re.sub(pattern4,\"Quran\",x)\n",
    "    x = re.sub(pattern5,\"Concerned\",x)\n",
    "    x = re.sub(pattern6,\"you guys\",x)\n",
    "    x = re.sub(pattern7,\"governments\",x)\n",
    "    x = re.sub(pattern8,\"you know\",x)\n",
    "        \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1804874/1804874 [02:13<00:00, 13493.12it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97320/97320 [00:07<00:00, 13580.49it/s]\n"
     ]
    }
   ],
   "source": [
    "train['comment_text'] = train['comment_text'].progress_apply(lambda x:fix_1(x))\n",
    "test['comment_text'] = test['comment_text'].progress_apply(lambda x:fix_1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing some identity related words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBGT -> lgbt - 106 occurences in train set\n",
    "# lieberal -> liberal - 83\n",
    "# Lieberal -> Liberal - 99 \n",
    "# realDonaldTrump -> Donald Trump 106 \n",
    "# theDonald -> Donald Trump 89\n",
    "# Trumpsters -> Trump supporters 340\n",
    "# Trumpian -> Trump supporters 350 \n",
    "\n",
    "def fix_identity(x):\n",
    "    pattern1 = r\"\\bLBGT\\b\"\n",
    "    pattern2 = r\"\\blieberal\\b\"\n",
    "    pattern3 = r\"\\bLieberal\\b\"\n",
    "    pattern4 = r\"\\brealDonaldTrump\\b\"\n",
    "    pattern5 = r\"\\btheDonald\\b\"\n",
    "    pattern6 = r\"\\bTrumpsters\\b\"\n",
    "    pattern7 = r\"\\bTrumpian\\b\"\n",
    " \n",
    "    x = re.sub(pattern1,\"LGBT\",x)\n",
    "    x = re.sub(pattern2,\"liberal\",x)\n",
    "    x = re.sub(pattern3,\"Liberal\",x)\n",
    "    x = re.sub(pattern4,\"Donald Trump\",x)\n",
    "    x = re.sub(pattern5,\"Donald Trump\",x)\n",
    "    x = re.sub(pattern6,\"Trump supporters\",x)\n",
    "    x = re.sub(pattern7,\"Trump supporter\",x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1804874/1804874 [01:55<00:00, 15632.36it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97320/97320 [00:06<00:00, 15684.76it/s]\n"
     ]
    }
   ],
   "source": [
    "train['comment_text'] = train['comment_text'].progress_apply(lambda x:fix_identity(x))\n",
    "test['comment_text'] = test['comment_text'].progress_apply(lambda x:fix_identity(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dependent and independent variables\n",
    "\n",
    "comment_text = train['comment_text']\n",
    "\n",
    "target = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_UNITS = 128\n",
    "max_features = 473670\n",
    "DENSE_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220\n",
    "EMBEDDING_DIM = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instance of keras Tokenizer class\n",
    "tokenizer = Tokenizer(num_words = max_features, filters='',lower=False)\n",
    "tokenizer.fit_on_texts(comment_text)\n",
    "\n",
    "# pad sequences to MAX_LEN\n",
    "comment_text = pad_sequences(tokenizer.texts_to_sequences(comment_text),MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[glove_matrix],\n",
    "                            input_length=MAX_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding = model.add(glove_layer)\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Bidirectional(LSTM(LSTM_UNITS*2,activation='tanh')))\n",
    "model.add(Dense(DENSE_UNITS,activation='relu'))\n",
    "model.add(Dense(DENSE_UNITS,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 220, 300)          142101300 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 220, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512)               1140736   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 143,767,861\n",
      "Trainable params: 1,666,561\n",
      "Non-trainable params: 142,101,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# setting target value to True/False\n",
    "\n",
    "target[target >= 0.5] = True\n",
    "target[target < 0.5] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples with this classes affiliation has more effect on evaluation metric\n",
    "\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting sample weights\n",
    "\n",
    "sample_weight = np.ones(train.shape[0]) # setting ordinary samples to weights of 1\n",
    "\n",
    "# set identity samples weights to (say) 3\n",
    "\n",
    "sample_weight[train['male'] > 0.5] = 3\n",
    "sample_weight[train['female'] > 0.5] = 3\n",
    "sample_weight[train['homosexual_gay_or_lesbian'] > 0.5] = 3\n",
    "sample_weight[train['christian'] > 0.5] = 3\n",
    "sample_weight[train['muslim'] > 0.5] = 3\n",
    "sample_weight[train['black'] > 0.5] = 3\n",
    "sample_weight[train['white'] > 0.5] = 3\n",
    "sample_weight[train['psychiatric_or_mental_illness'] > 0.5] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1804874/1804874 [==============================] - 1443s 800us/step - loss: 0.2091\n",
      "Epoch 2/4\n",
      "1804874/1804874 [==============================] - 1412s 782us/step - loss: 0.1729\n",
      "Epoch 3/4\n",
      "1804874/1804874 [==============================] - 1386s 768us/step - loss: 0.1644\n",
      "Epoch 4/4\n",
      "1804874/1804874 [==============================] - 1353s 750us/step - loss: 0.1580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f58161a25f8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(comment_text,target,batch_size=1024,epochs=4,sample_weight=sample_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\n",
    "\n",
    "submission['prediction'] = model.predict(pad_sequences(tokenizer.texts_to_sequences(test['comment_text']),MAX_LEN),batch_size=1024)[:, 0]\n",
    "submission.to_csv('submission.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
