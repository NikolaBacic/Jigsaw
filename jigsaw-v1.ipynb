{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jigsaw Unitended Bias in Toxic Classification - Kaggle competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of a problem\n",
    "\n",
    "*Can you help detect toxic comments and minimize unintended model bias? That's your challenge in this competition.*\n",
    "\n",
    "*In this competition, challenge is to build a model that recognizes toxicity in comments and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. Develop strategies to reduce unintended bias in machine learning models, and you'll help the Conversation AI team, and the entire industry, build models that work well for a wide range of conversations.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "This competition use a newly developed metric that combines several submetrics to balance overall performance with various aspects of unintended bias.\n",
    "\n",
    "First, lets define each submetric.\n",
    "\n",
    "#### Overall AUC\n",
    "This is the ROC-AUC for the full evaluation set.\n",
    "\n",
    "#### Bias AUCs:\n",
    "To measure unintended bias, we again calculate the ROC-AUC, this time on three specific subsets of the test set for each identity, each capturing a different aspect of unintended bias.\n",
    "\n",
    "#### Subgroup AUC\n",
    "Here, we restrict the data set to only the examples that mention the specific identity subgroup. A low value in this metric means the model does a poor job of distinguishing between toxic and non-toxic comments that mention the identity.\n",
    "\n",
    "#### BPSN (Background Positive, Subgroup Negative) AUC: \n",
    "Here, we restrict the test set to the non-toxic examples that mention the identity and the toxic examples that do not. A low value in this metric means that the model confuses non-toxic examples that mention the identity with toxic examples that do not, likely meaning that the model predicts higher toxicity scores than it should for non-toxic examples mentioning the identity.\n",
    "\n",
    "#### BNSP (Background Negative, Subgroup Positive) AUC:\n",
    "Here, we restrict the test set to the toxic examples that mention the identity and the non-toxic examples that do not. A low value here means that the model confuses toxic examples that mention the identity with non-toxic examples that do not, likely meaning that the model predicts lower toxicity scores than it should for toxic examples mentioning the identity.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Generalized Mean of Bias AUCs\n",
    "\n",
    "To combine the per-identity Bias AUCs into one overall measure, we calculate their generalized mean as defined below:\n",
    "\n",
    "\n",
    "<center>\n",
    "$M_{p}(m_{s})  =  (\\frac{1}{N}\\sum_{s=1}^{N} m^{p}_{s})^{\\frac{1}{p}}$\n",
    "</center>\n",
    "\n",
    "where:\n",
    "* $M_{p}$ - the pth-power mean function\n",
    "* $m_{s}$ - the bias metric m calculated for subgroup s\n",
    "* N = number of identity subgroups\n",
    "\n",
    "For this competition, we use a p value of -5 to encourage improvements of the models for the identity subgroups with the lowest model performance.\n",
    "\n",
    "### Final metric\n",
    "\n",
    "We combine the overall AUC with the generalized mean of the Bias AUCs to calculate the final model score:\n",
    "\n",
    "<center>\n",
    "$score = 0.25 * AUC_{overall} + \\sum_{a=1}^{A}(0.25*M_{p}(m_{s,a}))$\n",
    "</center>\n",
    "\n",
    "where, \n",
    "\n",
    "* A = 3 (number of submetrics)\n",
    "* $ m_{s,a}$ = bias metric for identity subgroup s using submetric a\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove-global-vectors-for-word-representation', 'jigsaw-unintended-bias-in-toxicity-classification']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import sys\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Embedding, Bidirectional, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run of this cell took: 10 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "df_raw = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv',usecols= ['target'] + ['comment_text'] + identity_columns)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Run of this cell took: \" + str(round(toc-tic)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>black</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>jewish</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target  ...  white\n",
       "0  0.000000  ...    NaN\n",
       "1  0.000000  ...    NaN\n",
       "2  0.000000  ...    NaN\n",
       "3  0.000000  ...    NaN\n",
       "4  0.893617  ...    0.0\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#setting target column to either 0 or 1\n",
    "threshold = 0.5\n",
    "for col in ['target'] + identity_columns:\n",
    "    df_raw[col][df_raw[col] < threshold] = 0\n",
    "    df_raw[col][df_raw[col] >= threshold] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract dependent and independent variables\n",
    "\n",
    "comment_text = df_raw['comment_text']\n",
    "\n",
    "target = df_raw['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run of this cell took: 194 seconds\n",
      "Found 397708 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#tokenize comment_text (dependent variable)\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "#choose vocabulary and max sequence size\n",
    "VOCABULARY_SIZE = 20000 \n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "\n",
    "#create instance of keras Tokenizer class\n",
    "tokenizer = Tokenizer(num_words=VOCABULARY_SIZE)\n",
    "tokenizer.fit_on_texts(comment_text)\n",
    "\n",
    "# pad sequences to MAX_SEQUENCE_LENGTH\n",
    "comment_text = pad_sequences(tokenizer.texts_to_sequences(comment_text),MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Run of this cell took: \" + str(round(toc-tic)) + \" seconds\")\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples is : 1700000\n",
      "Number of validation examples is : 104874\n"
     ]
    }
   ],
   "source": [
    "# separating data into train/validation sets\n",
    "\n",
    "train_text = comment_text[:1700000]\n",
    "train_target = target[:1700000]\n",
    "\n",
    "validate_text = comment_text[1700000:]\n",
    "validate_target = target[1700000:]\n",
    "\n",
    "print(\"Number of training examples is : \" + str(train_target.shape[0]))\n",
    "print(\"Number of validation examples is : \" + str(validate_target.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the pretrained embedding layer (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Next, we compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings:\n",
    "EMBEDDINGS_PATH = '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDINGS_PATH)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load this embedding_matrix into a layer\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#adding layers\n",
    "embedding = model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(166,activation='tanh')))\n",
    "model.add(Dense(80,activation='tanh'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1700000/1700000 [==============================] - 497s 292us/step - loss: 0.1832\n",
      "Epoch 2/7\n",
      "1700000/1700000 [==============================] - 493s 290us/step - loss: 0.1429\n",
      "Epoch 3/7\n",
      "1700000/1700000 [==============================] - 493s 290us/step - loss: 0.1354\n",
      "Epoch 4/7\n",
      "1700000/1700000 [==============================] - 493s 290us/step - loss: 0.1307\n",
      "Epoch 5/7\n",
      "1700000/1700000 [==============================] - 493s 290us/step - loss: 0.1277\n",
      "Epoch 6/7\n",
      "1700000/1700000 [==============================] - 493s 290us/step - loss: 0.1251\n",
      "Epoch 7/7\n",
      "1700000/1700000 [==============================] - 494s 290us/step - loss: 0.1229\n",
      "Run of this cell took: 3457 seconds\n"
     ]
    }
   ],
   "source": [
    "#fit the model\n",
    "tic = time.time()\n",
    "\n",
    "model.fit(train_text,train_target,batch_size=1024*4,epochs=7)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Run of this cell took: \" + str(round(toc-tic)) + \" seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate train and validation score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class for evaluation metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawEvaluator:\n",
    "\n",
    "    def __init__(self, y_true, y_identity, power=-5, overall_model_weight=0.25):\n",
    "        self.y = (y_true >= 0.5).astype(int)\n",
    "        self.y_i = (y_identity >= 0.5).astype(int)\n",
    "        self.n_subgroups = self.y_i.shape[1]\n",
    "        self.power = power\n",
    "        self.overall_model_weight = overall_model_weight\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_auc(y_true, y_pred):\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    def _compute_subgroup_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] == 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bpsn_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y == 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bnsp_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y != 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def compute_bias_metrics_for_model(self, y_pred):\n",
    "        records = np.zeros((3, self.n_subgroups))\n",
    "        for i in range(self.n_subgroups):\n",
    "            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n",
    "            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n",
    "            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n",
    "        return records\n",
    "\n",
    "    def _calculate_overall_auc(self, y_pred):\n",
    "        return roc_auc_score(self.y, y_pred)\n",
    "\n",
    "    def _power_mean(self, array):\n",
    "        total = sum(np.power(array, self.power))\n",
    "        return np.power(total / len(array), 1 / self.power)\n",
    "\n",
    "    def get_final_metric(self, y_pred):\n",
    "        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n",
    "        bias_score = np.average([\n",
    "            self._power_mean(bias_metrics[0]),\n",
    "            self._power_mean(bias_metrics[1]),\n",
    "            self._power_mean(bias_metrics[2])\n",
    "        ])\n",
    "        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n",
    "        bias_score = (1 - self.overall_model_weight) * bias_score\n",
    "        return overall_score + bias_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9202662695278806\n",
      "Run of this cell took: 17 seconds\n"
     ]
    }
   ],
   "source": [
    "# identity_columns = [\n",
    "#     'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "#     'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "# TRAIN SCORE\n",
    "# calculate in on first n examples (it should be close enough approximation)\n",
    "\n",
    "tic = time.time()\n",
    "n = 100000\n",
    "\n",
    "y_true = train_target.values[:n]\n",
    "y_identity = df_raw[identity_columns].iloc[:n].values\n",
    "y_pred = model.predict(train_text[:n],batch_size=1024)\n",
    "\n",
    "# evaluate\n",
    "evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "auc_test_score = evaluator.get_final_metric(y_pred)\n",
    "print(auc_test_score)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Run of this cell took: \" + str(round(toc-tic)) + \" seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9071720077108385\n",
      "Run of this cell took: 17 seconds\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION SCORE\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "y_true = validate_target.values\n",
    "y_identity = df_raw[identity_columns].iloc[1700000:].values\n",
    "y_pred = model.predict(validate_text,batch_size=1024)\n",
    "\n",
    "# evaluate\n",
    "evaluator = JigsawEvaluator(y_true, y_identity)\n",
    "auc_test_score = evaluator.get_final_metric(y_pred)\n",
    "print(auc_test_score)\n",
    "\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Run of this cell took: \" + str(round(toc-tic)) + \" seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a little bit more on valdiation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "104874/104874 [==============================] - 30s 288us/step - loss: 0.1372\n",
      "Epoch 2/5\n",
      "104874/104874 [==============================] - 30s 288us/step - loss: 0.1309\n",
      "Epoch 3/5\n",
      "104874/104874 [==============================] - 30s 288us/step - loss: 0.1261\n",
      "Epoch 4/5\n",
      "104874/104874 [==============================] - 30s 288us/step - loss: 0.1214\n",
      "Epoch 5/5\n",
      "104874/104874 [==============================] - 30s 288us/step - loss: 0.1167\n",
      "Run of this cell took: 151 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "model.fit(validate_text,validate_target,batch_size=1024*4,epochs=5)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Run of this cell took: \" + str(round(toc-tic)) + \" seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run of this cell took: 21 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "submission['prediction'] = model.predict(pad_sequences(tokenizer.texts_to_sequences(df_test['comment_text']),MAX_SEQUENCE_LENGTH),batch_size=1024)[:, 0]\n",
    "submission.to_csv('submission.csv')\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Run of this cell took: \" + str(round(toc-tic)) + \" seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "#### First try:\n",
    "\n",
    "Text preprocessing: no text preprocessed (only default tokenization)\n",
    "\n",
    "Model Architecture -> vocabulary size : 20000 sequence size = 150\n",
    "\n",
    "Layers:\n",
    "Embedding(vocabulary_size,300))\n",
    "Bidirectional(LSTM(166)))\n",
    "(Dense(80,activation='tanh'))\n",
    "(Dense(1,activation='sigmoid'))\n",
    "\n",
    "optimizer = adam, epochs = 5\n",
    "\n",
    "| Set  |Score|  \n",
    "|---|---|\n",
    "|  Train | 0.91871 | \n",
    "|  Validate | 0.90468 |  \n",
    "|   Test|  0.90673 |\n",
    "\n",
    "More detailed diagnostics on validation set:\n",
    "\n",
    "|  Identity | Subgroup AUC  | BPSN  | BNSP  | \n",
    "|---|---|---|---|---|\n",
    "| male  | 0.901 |0.886   | 0.958 |\n",
    "| female  |  0.897 | 0.898 | 0.951  | \n",
    "| homosexual_gay_or_lesbian  |  0.842 |  0.795 | 0.971  |  \n",
    "|  christian |  0.920 |0.930  | 0.941  | \n",
    "| jewish  | 0.867  | 0.904 | 0.926  |   \n",
    "|  muslim | 0.860  | 0.842 | 0.962  |\n",
    "| black  | 0.825  |  0.800 | 0.965  |\n",
    "| white  | 0.831  | 0.805  |  0.964 |   \n",
    "| psychiatric_or_mental_illness  | 0.872  | 0.850  | 0.963  |\n",
    "<br>\n",
    "* BPSN is the lowest score. A low value in this metric means that the model confuses non-toxic examples that mention the identity with toxic examples that do not, likely meaning that the model predicts higher toxicity scores than it should for non-toxic examples mentioning the identity.\n",
    "* model does good on BNSP scores meaning that model doesn't confuse toxic examples that mention the identity with non-toxic examples that do not"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
