{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original notebook: -> https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "Two rules:\n",
    "\n",
    "* Don't use standard preprocessing steps like stemming or stopword removal when you have pre-trained embeddings\n",
    "(You loose valuable information, which would help your NN to figure things out)\n",
    "* Get your vocabulary as close to the embeddings as possible\n",
    "\n",
    "I will use the GoogleNews pretrained embeddings.\n",
    "\n",
    "We start with a neat little trick that enables us to see a progressbar when applying functions to a pandas Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1000000, 2)\n",
      "Test shape :  (97320, 1)\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv',usecols = ['target'] + ['comment_text'], nrows = 1000000)\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv',usecols = ['comment_text'])\n",
    "\n",
    "print(\"Train shape : \", train.shape)\n",
    "print(\"Test shape : \", test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the following function to track our training vocabulary, which goes through all our text and counts the occurance of the contained words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    sentences: list of list of words\n",
    "    return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets populate the vocabulary and display the first 5 elements and their count. Note that now we can use progress_apply to see progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:11<00:00, 84125.39it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = train['comment_text'].progress_apply(lambda x: x.split()).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:14<00:00, 68711.83it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': 68488, 'is': 820186, 'so': 120264, 'cool.': 263, \"It's\": 45347}\n"
     ]
    }
   ],
   "source": [
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the embeddings we want to use in our model later. For illustration I use GoogleNews here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "news_path = '../input/quora-insincere-questions-classification/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I define a function that checks the intersection between our vocabulary and the embeddings. It will output a list of out of vocabulary (oov) words that we can use to improve our preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0 #counts number of words(tokens) we can represent with embedings\n",
    "    i = 0 #counts number of words(tokens) we can't represent with embedings\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1134573/1134573 [00:03<00:00, 335000.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 13.21% of vocab\n",
      "Found embeddings for  76.76% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch only 13.21% of our vocabulary will have embeddings, making ~23% of our data more or less useless. So lets have a look and start improving. For this we can easily have a look at the top oov words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 1480652),\n",
       " ('and', 1195130),\n",
       " ('of', 1155960),\n",
       " ('a', 1062940),\n",
       " ('-', 100234),\n",
       " ('.', 41336),\n",
       " (',', 17071),\n",
       " ('it,', 16371),\n",
       " ('that.', 15419),\n",
       " ('--', 15124)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On first place there is \"to\". Why? Simply because \"to\" was removed when the GoogleNews Embeddings were trained. We will fix this later, for now we take care about the splitting of punctuation as this also seems to be a Problem. But what do we do with the punctuation then - Do we want to delete or consider as a token? I would say: It depends. If the token has an embedding, keep it, if it doesn't we don't need it anymore. So lets check:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'?' in embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'&' in embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. While \"&\" is in the Google News Embeddings, \"?\" is not. So we basically define a function that splits off \"&\" and removes other punctuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:16<00:00, 62420.00it/s]\n",
      "100%|██████████| 1000000/1000000 [00:13<00:00, 76110.06it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"comment_text\"] = train[\"comment_text\"].progress_apply(lambda x: clean_text(x))\n",
    "sentences = train[\"comment_text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410450/410450 [00:01<00:00, 333179.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 41.61% of vocab\n",
      "Found embeddings for  89.25% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We were able to increase our embeddings ratio from 13.21% to 41.61% by just handling punctiation. Ok lets check on those oov words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 1495671),\n",
       " ('and', 1218100),\n",
       " ('of', 1164992),\n",
       " ('a', 1071602),\n",
       " ('10', 15404),\n",
       " ('20', 12068),\n",
       " ('100', 10495),\n",
       " ('2016', 9909),\n",
       " ('50', 9740),\n",
       " ('30', 9022)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm seems like numbers also are a problem. Lets check the top 10 embeddings to get a clue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n",
      "in\n",
      "for\n",
      "that\n",
      "is\n",
      "on\n",
      "##\n",
      "The\n",
      "with\n",
      "said\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(embeddings_index.index2entity[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm why is \"##\" in there? Simply because as a reprocessing all numbers bigger than 9 have been replaced by hashs. I.e. 15 becomes ## while 123 becomes ### or 15.80€ becomes ##.##€. So lets mimic this preprocessing step to further improve our embeddings coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_numbers(x):\n",
    "\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:30<00:00, 33159.27it/s]\n",
      "100%|██████████| 1000000/1000000 [00:06<00:00, 145481.50it/s]\n",
      "100%|██████████| 1000000/1000000 [00:12<00:00, 77360.59it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"comment_text\"] = train[\"comment_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "sentences = train[\"comment_text\"].progress_apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393973/393973 [00:01<00:00, 331467.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 43.61% of vocab\n",
      "Found embeddings for  89.96% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Another ~2.5% increase. Now as much as with handling the puntuation, but every bit helps. Lets check the oov words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 1495671),\n",
       " ('and', 1218100),\n",
       " ('of', 1164992),\n",
       " ('a', 1071602),\n",
       " ('–', 3637),\n",
       " ('—', 3525),\n",
       " ('wwwyoutubecom', 2716),\n",
       " ('judgement', 1844),\n",
       " ('behaviour', 1756),\n",
       " ('favour', 1699),\n",
       " ('tRump', 1681),\n",
       " ('labour', 1636),\n",
       " ('doesnt', 1580),\n",
       " ('didnt', 1568),\n",
       " ('enwikipediaorg', 1525),\n",
       " ('Brexit', 1325),\n",
       " ('defence', 1092),\n",
       " ('centre', 1047),\n",
       " ('isnt', 1022),\n",
       " ('wwwadncom', 882)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now we take care of common misspellings when using american/ british vocab and replacing a few \"modern\" words with \"social media\" for this task I use a multi regex script I found some time ago on stackoverflow. Additionally we will simply remove the words \"a\",\"to\",\"and\" and \"of\" since those have obviously been downsampled when training the GoogleNews Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'isnt':'is not',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'labour':'labor',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium'\n",
    "                }\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:12<00:00, 81622.80it/s]\n",
      "100%|██████████| 1000000/1000000 [00:08<00:00, 115198.45it/s]\n",
      "100%|██████████| 1000000/1000000 [00:10<00:00, 93088.75it/s]\n",
      "100%|██████████| 1000000/1000000 [00:12<00:00, 81185.14it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"comment_text\"] = train[\"comment_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "sentences = train[\"comment_text\"].progress_apply(lambda x: x.split())\n",
    "to_remove = ['a','to','of','and']\n",
    "sentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393918/393918 [00:01<00:00, 321267.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 43.62% of vocab\n",
      "Found embeddings for  99.13% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we improved on the amount of embeddings found for all our text from 89% to 99%. Lets check the oov words again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('–', 3637),\n",
       " ('—', 3525),\n",
       " ('wwwyoutubecom', 2716),\n",
       " ('judgement', 1844),\n",
       " ('behaviour', 1756),\n",
       " ('favour', 1699),\n",
       " ('tRump', 1681),\n",
       " ('enwikipediaorg', 1525),\n",
       " ('Brexit', 1325),\n",
       " ('defence', 1092),\n",
       " ('wwwadncom', 882),\n",
       " ('…', 789),\n",
       " ('article#####', 687),\n",
       " ('wwwnytimescom', 683),\n",
       " ('wwwtheglobeandmailcom', 679),\n",
       " ('wwwwashingtonpostcom', 678),\n",
       " ('neighbours', 629),\n",
       " ('hominem', 604),\n",
       " ('deplorables', 589),\n",
       " ('Drumpf', 576)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
